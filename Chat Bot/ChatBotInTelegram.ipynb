{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Чат-бот для Telegram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Исходные данные:\n",
    "1. Словарь намерений BOT_CONFIG на тему еды и некоторые общие темы (т.е. у бота можно, например, спросить: \"Что можно быстро приготовить?\" или \"Что можно приготовить к макаронам? и т.д.)\n",
    "2. Файл dialogues.txt, представляющий собой сборник диалогов из художественной литературы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Архитектура чат-бота:\n",
    "1. Возвращаем заготовленный ответ, извлекаемый из словаря намерений BOT_CONFIG.\n",
    "2. Используем генеративную модель в том случае, если не находим подходящего намерения.\n",
    "3. Используем ответ-заглушку, если заготовленный ответ и генеративная модель не дают результата."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для классификации намерений будем использовать классификаторы LogisticRegression и LinearSVC. LogisticRegression используем для определения вероятности правильной классификации намерения. LinearSVC для нахождения лучшего намерения. Применение двух разных классификаторов связано с тем, что LinearSVC показывает более высокую точность классификации, чем LogisticRegression. Но, в отличие от последнего, не имеет метода predict_proba() (возвращает вероятностные оценки), необходимого для проверки адекватности классикации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для генерации ответа будем использовать расстояние Левенштейна для определения наиболее подходящего ответа."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Различные варианты ответов-заглушек будем брать из того же BOT_CONFIG, находящихся там под ключом failure_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('BOT_CONFIG.pickle', 'rb') as f:\n",
    "    BOT_CONFIG = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала необходимо векторизовать все наши намерения, т.е. преобразовать фразы в некоторые числовые последовательности, которые будут \"понятны\" классификаторам.\n",
    "Будем использовать TfidfVectorizer с параметрами analyzer='char' и ngram_range=(2, 3), т.е. будем разбивать фразы на n-граммы по 2 или 3 буквы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подготавливаем данные для векторизации\n",
    "corpus = []\n",
    "y = []\n",
    "for intent, intent_data in BOT_CONFIG['intents'].items():\n",
    "    for example in intent_data['examples']:\n",
    "        corpus.append(example)\n",
    "        y.append(intent)\n",
    "\n",
    "# Векторизируем с помощью tf-idf векторайзера \n",
    "vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(2, 3))\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как было сказано выше, используем логистическую регрессию для определения вероятности правильной классификации намерения. Будем использовать RandomizedSearchCV для подбора лучших в данном случае параметров классификатора. Также используем кросс-валидацию, которая даст нам более верный набор параметров, т.к. данные из тренировочного датасета будут дополнительно разбиты на n частей (параметр cv=n в RandomizedSearchCV), далее модель будет обучена на n-1 из них, затем протестирована на оставшейся (такое действие будет выполнено для каждой из n частей)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:670: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn((\"The least populated class in y has only %d\"\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3296398891966759"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Разбиваем исходный набор данных на тренировочный и тестовый датасеты\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "# Задаём параметры и их значения, которые необходимо перебрать\n",
    "parameters = {'tol': [1e-3, 1e-4, 1e-5, 1e-6], 'C': [0.5, 0.75, 1], 'fit_intercept': [False, True], 'random_state': [42],\n",
    "             'solver': ['newton-cg', 'lbfgs', 'sag', 'saga']}\n",
    "\n",
    "# Подбираем лучшие параметры\n",
    "clf_proba = LogisticRegression()\n",
    "rand_search_clf = RandomizedSearchCV(clf_proba, parameters, cv=5)\n",
    "rand_search_clf.fit(X_train, y_train)\n",
    "clf_proba = rand_search_clf.best_estimator_\n",
    "clf_proba.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Точность LogisticRegression составляет 34,1%, что вполне неплохо, учитывая очень низкое качество словаря намерений BOT_CONFIG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.75, fit_intercept=False, random_state=42, solver='saga',\n",
       "                   tol=1e-05)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Обучаем классификатор LogisticRegression на полном наборе данных\n",
    "clf_proba.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем метод опорных вукторов для нахождения лучшего намерения. Здесь также будем использовать RandomizedSearchCV для подбора лучших в данном случае параметров классификатора и кросс-валидацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:670: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn((\"The least populated class in y has only %d\"\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.40166204986149584"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Разбиваем исходный набор данных на тренировочный и тестовый датасеты\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "# Задаём параметры и их значения, которые необходимо перебрать\n",
    "parameters = {'tol': [1e-3, 1e-4, 1e-5, 1e-6], 'C': [0.5, 0.75, 1], 'random_state': [42], \n",
    "              'loss': ['hinge', 'squared_hinge'], 'multi_class': ['ovr', 'crammer_singer']}\n",
    "\n",
    "# Подбираем лучшие параметры\n",
    "clf = LinearSVC()\n",
    "rand_search_clf = RandomizedSearchCV(clf, parameters, cv=5)\n",
    "rand_search_clf.fit(X_train, y_train)\n",
    "clf = rand_search_clf.best_estimator_\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Точность LinearSVC составляет 40,2%. Низкая точность опять же объясняется низким качеством словаря намерений BOT_CONFIG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=0.75, random_state=42, tol=1e-05)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Обучаем классификатор LinearSVC на полном наборе данных\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее приведены две функции для получения заготовленного ответа."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intent(question):\n",
    "    \"\"\"Предсказываем лучший intent и проверяем адекватность его классификации \n",
    "        по минимальной вероятности (минимальному порогу)\"\"\"\n",
    "    \n",
    "    # Предсказываем лучшее намерение с помощью LinearSVC\n",
    "    best_intent = clf.predict(vectorizer.transform([question]))[0]\n",
    "\n",
    "    # Находим вероятность правильной классификации намерения\n",
    "    index_of_best_intent = list(clf_proba.classes_).index(best_intent)\n",
    "    probabilities = clf_proba.predict_proba(vectorizer.transform([question]))[0]\n",
    "    \n",
    "    # проверяем адекватность классификации по минимальной вероятности (минимальному порогу)\n",
    "    min_treshold = 0.05\n",
    "    best_intent_proba = probabilities[index_of_best_intent]\n",
    "    if best_intent_proba > min_treshold:\n",
    "        return best_intent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer_by_intent(intent):\n",
    "    \"\"\"Получаем случайный из возможных ответ на намерение\"\"\"\n",
    "    phrases = BOT_CONFIG['intents'][intent]['responses']\n",
    "    return random.choice(phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь подготавливаем данные для генеративной модели. Создаём датасет вида {word1: [[question1, answer1], [question2, aanswer2], ...], ...}. После чего удаляем самые популярные слова, т.к. они встречаются почти в каждой фразе, а значит не имеют значения при классификации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подготавливаем данные для генеративной модели\n",
    "with open('dialogues.txt', encoding='utf8') as f:\n",
    "    content = f.read()\n",
    "\n",
    "dialogues = content.split('\\n\\n')\n",
    "\n",
    "def clear_question(question):\n",
    "    \"\"\"Очистка строки от лишних символов\"\"\"\n",
    "    question = question.lower().strip()\n",
    "    alphabet = ' -1234567890йцукенгшщзхъфывапролджэёячсмитьбю'\n",
    "    question = ''.join(c for c in question if c in alphabet)\n",
    "    return question\n",
    "\n",
    "# Создание датасета вида {word1: [[q1, a1], [q2, a2], ...], ...}\n",
    "questions = set()\n",
    "dataset = {}\n",
    "\n",
    "for dialogue in dialogues:\n",
    "    replicas = dialogue.split('\\n')[:2]\n",
    "    if len(replicas) == 2:\n",
    "        question, answer = replicas\n",
    "        question = clear_question(question[2:])\n",
    "        answer = answer[2:]\n",
    "        \n",
    "        if question and question not in questions:\n",
    "            questions.add(question)\n",
    "            words = question.split(' ')\n",
    "            for word in words:\n",
    "                if word not in dataset:\n",
    "                    dataset[word] = []\n",
    "                dataset[word].append([question, answer])\n",
    "\n",
    "# Удаляем самые популярные словы из датасета\n",
    "too_popular = set()\n",
    "for word in dataset:\n",
    "    if len(dataset[word]) > 10000:\n",
    "        too_popular.add(word)\n",
    "\n",
    "for word in too_popular:\n",
    "    dataset.pop(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее приведена функция для генерирования ответа. Здесь сначала создаём небольшой датасет, только с теми словами, которые входят в фразу, приходящую на вход. Делаем это для того, чтобы не перебирать каждый раз большой датасет (в некоторых случаях на это требуется много времени). После чего возвращаем ответ с минимальным расстоянием Левенштейна (расстояние Левенштейна находим с помощью функции edit_distance() библиотеки nltk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_generative_answer(replica):\n",
    "    \"\"\"Получаем ответ, основанный на генеративной модели\"\"\"\n",
    "    \n",
    "    # Очищаем входную фразу от лишних символов\n",
    "    replica = clear_question(replica)\n",
    "    words = replica.split(' ')\n",
    "    \n",
    "    # Формируем небольшой датасет\n",
    "    mini_dataset = []\n",
    "    for word in words:\n",
    "        if word in dataset:\n",
    "            mini_dataset += dataset[word]\n",
    "\n",
    "    # Находим минимальное расстояние Левенштейна\n",
    "    candidates = []\n",
    "    for question, answer in mini_dataset:\n",
    "        if abs(len(question) - len(replica)) / len(question) < 0.4:\n",
    "            d = nltk.edit_distance(question, replica)\n",
    "            diff = d / len(question)\n",
    "            if diff < 0.4:\n",
    "                candidates.append([question, answer, diff])\n",
    "    \n",
    "    if candidates: \n",
    "        winner = min(candidates, key=lambda candidate: candidate[2])\n",
    "        return winner[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее приведена функция для получения ответа-заглушки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_failure_phrase():\n",
    "    \"\"\"Получаем случайный ответ-заглушку\"\"\"\n",
    "    phrases = BOT_CONFIG['failure_phrases']\n",
    "    return random.choice(phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее приведена основная функция бота."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bot(question):\n",
    "    #\n",
    "    # NLU\n",
    "    intent = get_intent(question)\n",
    "\n",
    "    #\n",
    "    # Получение ответа\n",
    "\n",
    "    # Заготовленный ответ\n",
    "    if intent:\n",
    "        return get_answer_by_intent(intent)\n",
    "\n",
    "    # Применяем генеративную модель\n",
    "    answer = get_generative_answer(question)\n",
    "    if answer:\n",
    "        return answer\n",
    "\n",
    "    # Ответ-заглушка\n",
    "    return get_failure_phrase()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее приведен код простейшего Telegram бота. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install python-telegram-bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {'token': '!!!Insert your Telegram bot token!!!'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from telegram.ext import Updater, CommandHandler, MessageHandler, Filters\n",
    "\n",
    "\n",
    "def start(update, context):\n",
    "    \"\"\"Send a message when the command /start is issued.\"\"\"\n",
    "    update.message.reply_text('Hi!')\n",
    "\n",
    "\n",
    "def help_command(update, context):\n",
    "    \"\"\"Send a message when the command /help is issued.\"\"\"\n",
    "    update.message.reply_text('Help!')\n",
    "\n",
    "\n",
    "def echo(update, context):\n",
    "    \"\"\"Echo the user message.\"\"\"\n",
    "    question = update.message.text\n",
    "    answer = bot(question)\n",
    "    update.message.reply_text(answer)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Start the bot.\"\"\"\n",
    "    updater = Updater(CONFIG['token'], use_context=True)\n",
    "\n",
    "    dp = updater.dispatcher\n",
    "    dp.add_handler(CommandHandler(\"start\", start))\n",
    "    dp.add_handler(CommandHandler(\"help\", help_command))\n",
    "    dp.add_handler(MessageHandler(Filters.text & ~Filters.command, echo))\n",
    "\n",
    "    updater.start_polling()\n",
    "    updater.idle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
